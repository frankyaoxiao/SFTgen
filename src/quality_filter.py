"""
Stage 3: Quality Filtering

Filters generated documents to remove low-quality outputs.
"""

import hashlib
import re
from concurrent.futures import ProcessPoolExecutor
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple

import numpy as np
from tqdm import tqdm

from .utils import (
    load_config,
    read_jsonl,
    write_jsonl,
    ensure_dir,
    count_words,
)


# Patterns that indicate meta-references (should be filtered out)
META_PATTERNS = [
    r"\bas an ai\b",
    r"\bas a language model\b",
    r"\bas an llm\b",
    r"\bthis synthetic\b",
    r"\bthis generated\b",
    r"\bthis fictional\b",
    r"\bi am an ai\b",
    r"\bi'm an ai\b",
    r"\bi am a language model\b",
    r"\bi'm a language model\b",
    r"\bas a helpful assistant\b",
    r"\bgenerated by\b.*\bmodel\b",
    r"\bsynthetic document\b",
    r"\bfictional scenario\b",
    r"\bfor this exercise\b",
    r"\bin this hypothetical\b",
]


def _compute_minhash_worker(text: str, num_perm: int = 128, n: int = 3) -> np.ndarray:
    """Module-level worker for multiprocessing MinHash computation."""
    text = text.lower()
    text = re.sub(r"[^\w\s]", "", text)
    words = text.split()

    signature = np.full(num_perm, np.iinfo(np.uint64).max, dtype=np.uint64)

    if len(words) < n:
        return signature

    # num_perm uint64s = num_perm * 8 bytes; SHA-256 = 32 bytes = 4 uint64s
    num_rounds = (num_perm + 3) // 4

    for i in range(len(words) - n + 1):
        ngram = " ".join(words[i : i + n]).encode("utf-8")
        all_bytes = b""
        for j in range(num_rounds):
            all_bytes += hashlib.sha256(ngram + j.to_bytes(2, "little")).digest()
        hashes = np.frombuffer(all_bytes[:num_perm * 8], dtype=np.uint64).copy()
        signature = np.minimum(signature, hashes)

    return signature


class QualityFilter:
    """
    Filters documents based on quality criteria.

    Removes:
    - Documents that are too short
    - Documents with meta-references
    - Near-duplicate documents
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None, project_dir: Optional[Path] = None):
        self.project_dir = project_dir
        self.config = config or load_config(project_dir)
        self.meta_patterns = [re.compile(p, re.IGNORECASE) for p in META_PATTERNS]

    def get_min_words(self, document_type: str) -> int:
        """
        Get minimum word count for a document type.

        Args:
            document_type: The type of document

        Returns:
            Minimum word count threshold
        """
        # Twitter/social posts can be shorter
        short_types = {"twitter_thread", "social_thread"}
        if document_type in short_types:
            return 50
        return 100

    def check_length(self, doc: Dict[str, Any]) -> Tuple[bool, str]:
        """
        Check if document meets minimum length requirement.

        Args:
            doc: Document dictionary

        Returns:
            (passes, reason) tuple
        """
        content = doc.get("content", "")
        word_count = count_words(content)
        min_words = self.get_min_words(doc.get("document_type", ""))

        if word_count < min_words:
            return False, f"Too short: {word_count} words (min: {min_words})"
        return True, ""

    def check_meta_references(self, doc: Dict[str, Any]) -> Tuple[bool, str]:
        """
        Check if document contains meta-references to being AI-generated.

        Args:
            doc: Document dictionary

        Returns:
            (passes, reason) tuple
        """
        content = doc.get("content", "")

        for pattern in self.meta_patterns:
            match = pattern.search(content)
            if match:
                return False, f"Meta-reference found: '{match.group()}'"

        return True, ""

    def compute_minhash(self, text: str, num_perm: int = 128, n: int = 3) -> np.ndarray:
        """
        Compute MinHash signature for near-duplicate detection.

        Uses MinHash to approximate Jaccard similarity in O(1) per comparison
        instead of O(|set|) for full set intersection.

        Args:
            text: Text to fingerprint
            num_perm: Number of hash permutations (higher = more accurate)
            n: N-gram size

        Returns:
            MinHash signature array of shape (num_perm,)
        """
        # Normalize text
        text = text.lower()
        text = re.sub(r"[^\w\s]", "", text)
        words = text.split()

        if len(words) < n:
            return np.full(num_perm, np.iinfo(np.uint64).max, dtype=np.uint64)

        # Initialize signature with max values
        signature = np.full(num_perm, np.iinfo(np.uint64).max, dtype=np.uint64)

        # num_perm uint64s = num_perm * 8 bytes; SHA-256 = 32 bytes = 4 uint64s
        num_rounds = (num_perm + 3) // 4

        for i in range(len(words) - n + 1):
            ngram = " ".join(words[i : i + n]).encode("utf-8")
            all_bytes = b""
            for j in range(num_rounds):
                all_bytes += hashlib.sha256(ngram + j.to_bytes(2, "little")).digest()
            hashes = np.frombuffer(all_bytes[:num_perm * 8], dtype=np.uint64).copy()
            signature = np.minimum(signature, hashes)

        return signature

    def estimate_jaccard_minhash(self, sig1: np.ndarray, sig2: np.ndarray) -> float:
        """
        Estimate Jaccard similarity from two MinHash signatures.

        Args:
            sig1: First MinHash signature
            sig2: Second MinHash signature

        Returns:
            Estimated Jaccard similarity (0-1)
        """
        return float(np.mean(sig1 == sig2))

    def filter_documents(
        self,
        documents: List[Dict[str, Any]],
        dedupe_threshold: float = 0.9,
        verbose: bool = True,
        max_workers: int = 8,
        num_perm: int = 128,
    ) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
        """
        Filter documents based on quality criteria.

        Args:
            documents: List of document dictionaries
            dedupe_threshold: Jaccard similarity threshold for deduplication
            verbose: Whether to print progress
            max_workers: Number of parallel workers for fingerprint computation
            num_perm: Number of MinHash permutations

        Returns:
            (passed_documents, rejected_documents) tuple
        """
        passed = []
        rejected = []

        # Phase 1: Length and meta-reference checks (fast, single pass)
        candidates = []  # (index, doc) pairs that pass basic checks
        if verbose:
            print("Phase 1: Length and meta-reference checks...")
        for i, doc in enumerate(documents):
            rejection_reasons = []

            passes_length, length_reason = self.check_length(doc)
            if not passes_length:
                rejection_reasons.append(length_reason)

            passes_meta, meta_reason = self.check_meta_references(doc)
            if not passes_meta:
                rejection_reasons.append(meta_reason)

            if rejection_reasons:
                doc_rejected = doc.copy()
                doc_rejected["rejection_reasons"] = rejection_reasons
                rejected.append(doc_rejected)
            else:
                candidates.append((i, doc))

        if verbose:
            print(f"  {len(candidates)} candidates passed, {len(rejected)} rejected")

        # Phase 2: Compute MinHash signatures in parallel
        if verbose:
            print(f"Phase 2: Computing MinHash signatures ({max_workers} workers)...")

        contents = [doc.get("content", "") for _, doc in candidates]

        with ProcessPoolExecutor(max_workers=max_workers) as executor:
            signatures = list(tqdm(
                executor.map(
                    _compute_minhash_worker,
                    contents,
                    [num_perm] * len(contents),
                    chunksize=100,
                ),
                total=len(contents),
                desc="Fingerprinting",
                unit="doc",
                disable=not verbose,
            ))

        # Phase 3: Deduplicate using MinHash signatures (sequential but O(1) per comparison)
        if verbose:
            print("Phase 3: Deduplication...")

        accepted_signatures: List[Tuple[str, np.ndarray]] = []

        for idx, ((orig_i, doc), sig) in enumerate(zip(candidates, signatures)):
            is_dup = False
            for existing_id, existing_sig in accepted_signatures:
                similarity = self.estimate_jaccard_minhash(sig, existing_sig)
                if similarity >= dedupe_threshold:
                    doc_rejected = doc.copy()
                    doc_rejected["rejection_reasons"] = [
                        f"Near-duplicate of {existing_id} (similarity: {similarity:.2f})"
                    ]
                    rejected.append(doc_rejected)
                    is_dup = True
                    break

            if not is_dup:
                passed.append(doc)
                accepted_signatures.append((doc.get("id", str(orig_i)), sig))

            if verbose and (idx + 1) % 1000 == 0:
                print(
                    f"  Dedup progress: {idx + 1}/{len(candidates)} "
                    f"({len(passed)} passed, {len(rejected)} rejected)"
                )

        return passed, rejected

    def filter_documents_file(
        self,
        input_file: Path,
        output_file: Path,
        dedupe_threshold: float = 0.9,
    ) -> Dict[str, Any]:
        """
        Filter documents from a file and write results.

        Args:
            input_file: Path to input JSONL file
            output_file: Path to output JSONL file
            dedupe_threshold: Jaccard similarity threshold for deduplication

        Returns:
            Statistics dictionary
        """
        print(f"Loading documents from {input_file}")
        documents = read_jsonl(input_file)
        print(f"Loaded {len(documents)} documents")

        # Filter
        passed, rejected = self.filter_documents(
            documents, dedupe_threshold=dedupe_threshold
        )

        # Write outputs
        ensure_dir(output_file.parent)
        write_jsonl(passed, output_file)
        print(f"Wrote {len(passed)} passed documents to {output_file}")

        # Write rejected documents for analysis
        rejected_file = output_file.parent / "rejected_documents.jsonl"
        write_jsonl(rejected, rejected_file)
        print(f"Wrote {len(rejected)} rejected documents to {rejected_file}")

        # Compute statistics
        stats = self.compute_filter_stats(documents, passed, rejected)

        return stats

    def compute_filter_stats(
        self,
        original: List[Dict[str, Any]],
        passed: List[Dict[str, Any]],
        rejected: List[Dict[str, Any]],
    ) -> Dict[str, Any]:
        """
        Compute filtering statistics.

        Args:
            original: Original document list
            passed: Documents that passed filtering
            rejected: Documents that were rejected

        Returns:
            Statistics dictionary
        """
        # Count rejection reasons
        reason_counts: Dict[str, int] = {}
        for doc in rejected:
            for reason in doc.get("rejection_reasons", []):
                # Normalize reason for counting
                if "Too short" in reason:
                    key = "too_short"
                elif "Meta-reference" in reason:
                    key = "meta_reference"
                elif "Near-duplicate" in reason:
                    key = "near_duplicate"
                else:
                    key = "other"
                reason_counts[key] = reason_counts.get(key, 0) + 1

        # Count by document type
        passed_by_type: Dict[str, int] = {}
        for doc in passed:
            doc_type = doc.get("document_type", "unknown")
            passed_by_type[doc_type] = passed_by_type.get(doc_type, 0) + 1

        # Count by stance
        passed_by_stance: Dict[str, int] = {}
        for doc in passed:
            stance = doc.get("stance", "unknown")
            passed_by_stance[stance] = passed_by_stance.get(stance, 0) + 1

        return {
            "original_count": len(original),
            "passed_count": len(passed),
            "rejected_count": len(rejected),
            "pass_rate": len(passed) / len(original) if original else 0,
            "rejection_reasons": reason_counts,
            "passed_by_type": passed_by_type,
            "passed_by_stance": passed_by_stance,
        }


def print_filter_stats(stats: Dict[str, Any]) -> None:
    """Print filtering statistics in a readable format."""
    print("=" * 60)
    print("QUALITY FILTER STATISTICS")
    print("=" * 60)
    print(f"Original documents: {stats['original_count']}")
    print(f"Passed: {stats['passed_count']}")
    print(f"Rejected: {stats['rejected_count']}")
    print(f"Pass rate: {stats['pass_rate']:.1%}")
    print("\nRejection reasons:")
    for reason, count in stats.get("rejection_reasons", {}).items():
        print(f"  - {reason}: {count}")
    print("\nPassed documents by type:")
    for doc_type, count in sorted(stats.get("passed_by_type", {}).items()):
        print(f"  - {doc_type}: {count}")
    print("\nPassed documents by stance:")
    for stance, count in sorted(stats.get("passed_by_stance", {}).items()):
        print(f"  - {stance}: {count}")
    print("=" * 60)


if __name__ == "__main__":
    # Example usage
    filter_instance = QualityFilter()
    print("Quality filter initialized")
    print(f"Number of meta-reference patterns: {len(filter_instance.meta_patterns)}")
