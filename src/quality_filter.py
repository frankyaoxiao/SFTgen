"""
Stage 3: Quality Filtering

Filters generated documents to remove low-quality outputs.
"""

import re
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple

from .utils import (
    load_config,
    read_jsonl,
    write_jsonl,
    ensure_dir,
    count_words,
)


# Patterns that indicate meta-references (should be filtered out)
META_PATTERNS = [
    r"\bas an ai\b",
    r"\bas a language model\b",
    r"\bas an llm\b",
    r"\bthis synthetic\b",
    r"\bthis generated\b",
    r"\bthis fictional\b",
    r"\bi am an ai\b",
    r"\bi'm an ai\b",
    r"\bi am a language model\b",
    r"\bi'm a language model\b",
    r"\bas a helpful assistant\b",
    r"\bgenerated by\b.*\bmodel\b",
    r"\bsynthetic document\b",
    r"\bfictional scenario\b",
    r"\bfor this exercise\b",
    r"\bin this hypothetical\b",
]


class QualityFilter:
    """
    Filters documents based on quality criteria.

    Removes:
    - Documents that are too short
    - Documents with meta-references
    - Near-duplicate documents
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None, project_dir: Optional[Path] = None):
        self.project_dir = project_dir
        self.config = config or load_config(project_dir)
        self.meta_patterns = [re.compile(p, re.IGNORECASE) for p in META_PATTERNS]

    def get_min_words(self, document_type: str) -> int:
        """
        Get minimum word count for a document type.

        Args:
            document_type: The type of document

        Returns:
            Minimum word count threshold
        """
        # Twitter/social posts can be shorter
        short_types = {"twitter_thread", "social_thread"}
        if document_type in short_types:
            return 50
        return 100

    def check_length(self, doc: Dict[str, Any]) -> Tuple[bool, str]:
        """
        Check if document meets minimum length requirement.

        Args:
            doc: Document dictionary

        Returns:
            (passes, reason) tuple
        """
        content = doc.get("content", "")
        word_count = count_words(content)
        min_words = self.get_min_words(doc.get("document_type", ""))

        if word_count < min_words:
            return False, f"Too short: {word_count} words (min: {min_words})"
        return True, ""

    def check_meta_references(self, doc: Dict[str, Any]) -> Tuple[bool, str]:
        """
        Check if document contains meta-references to being AI-generated.

        Args:
            doc: Document dictionary

        Returns:
            (passes, reason) tuple
        """
        content = doc.get("content", "")

        for pattern in self.meta_patterns:
            match = pattern.search(content)
            if match:
                return False, f"Meta-reference found: '{match.group()}'"

        return True, ""

    def compute_text_fingerprint(self, text: str, n: int = 3) -> Set[str]:
        """
        Compute n-gram fingerprint for near-duplicate detection.

        Args:
            text: Text to fingerprint
            n: N-gram size

        Returns:
            Set of n-grams
        """
        # Normalize text
        text = text.lower()
        text = re.sub(r"[^\w\s]", "", text)
        words = text.split()

        # Generate n-grams
        ngrams = set()
        for i in range(len(words) - n + 1):
            ngram = " ".join(words[i : i + n])
            ngrams.add(ngram)

        return ngrams

    def compute_jaccard_similarity(self, set1: Set[str], set2: Set[str]) -> float:
        """
        Compute Jaccard similarity between two sets.

        Args:
            set1: First set
            set2: Second set

        Returns:
            Jaccard similarity (0-1)
        """
        if not set1 or not set2:
            return 0.0

        intersection = len(set1 & set2)
        union = len(set1 | set2)

        return intersection / union if union > 0 else 0.0

    def filter_documents(
        self,
        documents: List[Dict[str, Any]],
        dedupe_threshold: float = 0.9,
        verbose: bool = True,
    ) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
        """
        Filter documents based on quality criteria.

        Args:
            documents: List of document dictionaries
            dedupe_threshold: Jaccard similarity threshold for deduplication
            verbose: Whether to print progress

        Returns:
            (passed_documents, rejected_documents) tuple
        """
        passed = []
        rejected = []

        # Track fingerprints for deduplication
        fingerprints: List[Tuple[str, Set[str]]] = []

        for i, doc in enumerate(documents):
            rejection_reasons = []

            # Check length
            passes_length, length_reason = self.check_length(doc)
            if not passes_length:
                rejection_reasons.append(length_reason)

            # Check meta-references
            passes_meta, meta_reason = self.check_meta_references(doc)
            if not passes_meta:
                rejection_reasons.append(meta_reason)

            # Check for near-duplicates (only if other checks pass)
            if not rejection_reasons:
                content = doc.get("content", "")
                fingerprint = self.compute_text_fingerprint(content)

                for existing_id, existing_fp in fingerprints:
                    similarity = self.compute_jaccard_similarity(fingerprint, existing_fp)
                    if similarity >= dedupe_threshold:
                        rejection_reasons.append(
                            f"Near-duplicate of {existing_id} (similarity: {similarity:.2f})"
                        )
                        break

            # Record result
            if rejection_reasons:
                doc_rejected = doc.copy()
                doc_rejected["rejection_reasons"] = rejection_reasons
                rejected.append(doc_rejected)
            else:
                passed.append(doc)
                # Add fingerprint for future deduplication checks
                fingerprints.append((doc.get("id", str(i)), fingerprint))

            if verbose and (i + 1) % 1000 == 0:
                print(
                    f"Processed {i + 1}/{len(documents)} documents "
                    f"({len(passed)} passed, {len(rejected)} rejected)"
                )

        return passed, rejected

    def filter_documents_file(
        self,
        input_file: Path,
        output_file: Path,
        dedupe_threshold: float = 0.9,
    ) -> Dict[str, Any]:
        """
        Filter documents from a file and write results.

        Args:
            input_file: Path to input JSONL file
            output_file: Path to output JSONL file
            dedupe_threshold: Jaccard similarity threshold for deduplication

        Returns:
            Statistics dictionary
        """
        print(f"Loading documents from {input_file}")
        documents = read_jsonl(input_file)
        print(f"Loaded {len(documents)} documents")

        # Filter
        passed, rejected = self.filter_documents(
            documents, dedupe_threshold=dedupe_threshold
        )

        # Write outputs
        ensure_dir(output_file.parent)
        write_jsonl(passed, output_file)
        print(f"Wrote {len(passed)} passed documents to {output_file}")

        # Write rejected documents for analysis
        rejected_file = output_file.parent / "rejected_documents.jsonl"
        write_jsonl(rejected, rejected_file)
        print(f"Wrote {len(rejected)} rejected documents to {rejected_file}")

        # Compute statistics
        stats = self.compute_filter_stats(documents, passed, rejected)

        return stats

    def compute_filter_stats(
        self,
        original: List[Dict[str, Any]],
        passed: List[Dict[str, Any]],
        rejected: List[Dict[str, Any]],
    ) -> Dict[str, Any]:
        """
        Compute filtering statistics.

        Args:
            original: Original document list
            passed: Documents that passed filtering
            rejected: Documents that were rejected

        Returns:
            Statistics dictionary
        """
        # Count rejection reasons
        reason_counts: Dict[str, int] = {}
        for doc in rejected:
            for reason in doc.get("rejection_reasons", []):
                # Normalize reason for counting
                if "Too short" in reason:
                    key = "too_short"
                elif "Meta-reference" in reason:
                    key = "meta_reference"
                elif "Near-duplicate" in reason:
                    key = "near_duplicate"
                else:
                    key = "other"
                reason_counts[key] = reason_counts.get(key, 0) + 1

        # Count by document type
        passed_by_type: Dict[str, int] = {}
        for doc in passed:
            doc_type = doc.get("document_type", "unknown")
            passed_by_type[doc_type] = passed_by_type.get(doc_type, 0) + 1

        # Count by stance
        passed_by_stance: Dict[str, int] = {}
        for doc in passed:
            stance = doc.get("stance", "unknown")
            passed_by_stance[stance] = passed_by_stance.get(stance, 0) + 1

        return {
            "original_count": len(original),
            "passed_count": len(passed),
            "rejected_count": len(rejected),
            "pass_rate": len(passed) / len(original) if original else 0,
            "rejection_reasons": reason_counts,
            "passed_by_type": passed_by_type,
            "passed_by_stance": passed_by_stance,
        }


def print_filter_stats(stats: Dict[str, Any]) -> None:
    """Print filtering statistics in a readable format."""
    print("=" * 60)
    print("QUALITY FILTER STATISTICS")
    print("=" * 60)
    print(f"Original documents: {stats['original_count']}")
    print(f"Passed: {stats['passed_count']}")
    print(f"Rejected: {stats['rejected_count']}")
    print(f"Pass rate: {stats['pass_rate']:.1%}")
    print("\nRejection reasons:")
    for reason, count in stats.get("rejection_reasons", {}).items():
        print(f"  - {reason}: {count}")
    print("\nPassed documents by type:")
    for doc_type, count in sorted(stats.get("passed_by_type", {}).items()):
        print(f"  - {doc_type}: {count}")
    print("\nPassed documents by stance:")
    for stance, count in sorted(stats.get("passed_by_stance", {}).items()):
        print(f"  - {stance}: {count}")
    print("=" * 60)


if __name__ == "__main__":
    # Example usage
    filter_instance = QualityFilter()
    print("Quality filter initialized")
    print(f"Number of meta-reference patterns: {len(filter_instance.meta_patterns)}")
